\section{Datamining}

\subsection{Mining „auf“ Mobilgeräten}

\begin{frame}
    \frametitle{\insertsubsection}
    \begin{block}{Zwei Kategorien von mobilem Mining \cite{pocket2014}}
        \vspace{1em}
        „Mobile Interface“
        \vspace{0.6em}
        \begin{itemize}
            \setlength\itemsep{0.6em}
            \item Entfernter Mining Prozess (zB. Cloud)
            \item Mobiles Gerät als Zugang und Datenlieferant
        \end{itemize}
        \vspace{0.8em}
        „On-board Mining“
        \vspace{0.6em}
        \begin{itemize}
            \setlength\itemsep{0.6em}
            \item Mining Prozess auf dem Gerät
            \item Visualisierung (oder andere Auswertungen) auch auf Gerät
            \item $\rightarrow$ wachsende Leistungs-Anforderungen an mobile Geräte!
        \end{itemize}
    \end{block}
\end{frame}

\subsection{Mining Pozesse}

\begin{frame}
    \frametitle{\insertsubsection \ -- KDD}
    \begin{block}{Knowledge Discovery in Databases -- KDD \cite{kdd96}}
        \vspace{0.5em}
        \begin{itemize}
            \setlength\itemsep{1em}
            \item Selection
            \item Pre-processing
            \item Transformation
            \item Data Mining
            \item Interpretation/Evaluation
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{\insertsubsection \ -- KDD}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{img/KDD-DM_Process_Diagram}
        \caption{\cite{kdd96}}\label{fig:kdd}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{\insertsubsection \ -- CRISP}
    \begin{block}{Cross Industry Standard Process for Data Mining -- CRISP \cite{crisp2000}}
        \vspace{0.5em}
        \begin{itemize}
            \setlength\itemsep{1em}
            \item Business Understanding
            \item Data Understanding
            \item Data Preparation
            \item Modeling
            \item Evaluation
            \item Deployment
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{\insertsubsection \ -- CRISP}
    \begin{figure}[H]
        \centering
        \includegraphics[scale=2.2]{img/CRISP-DM_Process_Diagram}
        \caption{\url{https://commons.wikimedia.org/wiki/File\%3ACRISP-DM\_Process\_Diagram.png}\label{fig:crisp}}
    \end{figure}
\end{frame}

\subsection{Data Preparation}

\begin{frame}
    \frametitle{\insertsubsection}
    „Sehr aufwändiger, wenn nicht aufwändigster Teil-Prozess!” \cite{pocket2014}
    \vspace{1em}
    \begin{itemize}
        \setlength\itemsep{1em}
        \item Datenerhebung: Sammeln aus heterogenen Quellen
        \item Unifikation: Vereinheitlichen, soweit möglich
            \begin{itemize}
                \setlength\itemsep{0.5em}
                \item Formate bzw Attribute (ISO Standards, JSON, XML...)
                \item „Sparse Data” kann entstehen
            \end{itemize}
        \item „Outlier” Entfernen (nicht immer sinnvoll)
        \item „Preaggregation” -- zB. Mittelwerte verwenden (nicht immer sinnvoll)
    \end{itemize}
\end{frame}

\subsection{Learning Strategien}

\begin{frame}
    \frametitle{\insertsubsection \ \cite{datamining2011}}
    \begin{block}{„Supervised Learning“}
        \vspace{0.5em}
        \begin{itemize}
            \setlength\itemsep{0.5em}
            \item Klassifizierung \emph{vom Menschen / durch Datenpunkte} vorgegeben
            \item Input und (gewünschter) Output bekannt
            \item Algorithmen \emph{verwenden} Klassifikator
            \item Trainingsdaten \& Testdaten
        \end{itemize}
    \end{block}
    \vspace{0.8em}
    \begin{block}{„Unsupervised Learning“}
        \vspace{0.5em}
        \begin{itemize}
            \setlength\itemsep{0.5em}
            \item Klassifizierung \emph{vom Algorithmus} bestimmt
            \item Nur Input bekannt
            \item Output des Algorithmus muss Mensch zufrieden stellen
        \end{itemize}
    \end{block}
\end{frame}


\subsection{Linear Regression}

\begin{frame}
    \frametitle{\insertsubsection \ -- \cite{datamining2011}}
    Supervised $\rightarrow$ Klassifikation durch aufgestellte Formel anhand der Attribute bereits bekannter Punkte
    \vspace{1em}
    \begin{itemize}
        \setlength\itemsep{0.8em}
        \item Klasse $\approx$ lineare Kombination der Attribute (des zu klassifizierendes Punktes)
        \item Algorithmus lernt, Attribute zu wichten
        \item $f(x) = w_0 + w_1a_1 + w_2a_2 + ... + w_ka_k$
        \begin{itemize}
            \vspace{0.6em}
            \setlength\itemsep{0.6em}
            \item $f(x)$: Vorhersage-Funktion der Klasse
            \item $a_k$: Bekannte Attribute
            \item $w_k$: Wichtung der Attribute
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{K-Nearest-Neighbor (KNN)}

\begin{frame}
    \frametitle{\insertsubsection}
    Supervised $\rightarrow$ Klassifikation durch bereits bekannte Klassifikation der Nachbarn \cite{datamining2011}
    \begin{figure}[H]
        \centering
        \includegraphics[width=.5\textwidth]{img/knn}
        \caption{K-Nearest-Neighbor, vereinfachte Darstellung. Klassifikation der Roten Box: Wähle $K=5 \rightarrow$ \textit{blau}. Wähle $K=10 \rightarrow$ \textit{grün}\label{fig:knn}}
    \end{figure}
\end{frame}


\subsection{Support Vector Machine -- SVM}

\begin{frame}
    \frametitle{\insertsubsection \ -- \cite{datamining2011}}
    Supervised $\rightarrow$ Klassifikation durch bereits bekannte Klassifikation anderer Punkte 
    \vspace{1em}
    \begin{itemize}
        \setlength\itemsep{0.8em}
        \item Betrachtet eine Menge „kritischer Punkte”
        \item Punkte so weit wie möglich von einander entfernt unterteilen
        \item „Hyperplane“: Lineare Unterscheidungsfunktion, eine Dimension geringer als Features in Punktewolke
        \item $\rightarrow$ Binärer (linearer) Klassifikator
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{\insertsubsection}
     B klassifiziert „am besten”:
    \begin{figure}[H]
        \centering
        \includegraphics[width=.6\textwidth]{img/SVM_21}
        \caption{Online Material von \cite{ray2015}, \url{https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/}\label{fig:svm}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{\insertsubsection}
    Und nun?
    \begin{figure}[H]
        \centering
        \includegraphics[width=.6\textwidth]{img/SVM_5}
        \caption{Online Material von \cite{ray2015}, \url{https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/}\label{fig:svm2}}
    \end{figure}
\end{frame}

\subsection{K-Means Clustering}

\begin{frame}[fragile]
    \frametitle{\insertsubsection}
    Unsupervised $\rightarrow$ Einzelne Punkte haben keine bekannte Klassifikation \cite{datamining2011}
    \vspace{1em}
    \begin{lstlisting}
def K-Means(K: int, toCluster: Set):
    Place K centroids randomly
    repeat until convergence:
        for each Point in 'toCluster':
            find closest centroid
            add point to cluster
        for each cluster in 1..K:
            new centroid: mid of all points
    \end{lstlisting}
    \cite{lavrenko2013}
\end{frame}

\begin{frame}
    \frametitle{\insertsubsection}

    \begin{figure}[H]
        \centering
        \includegraphics[width=.6\textwidth]{img/kmeans}
        \caption{Online Material von \cite{lavrenko2013}, \url{https://i.ytimg.com/vi/_aWzGGNrcic/hqdefault.jpg}\label{fig:kmeans}}
    \end{figure}
\end{frame}