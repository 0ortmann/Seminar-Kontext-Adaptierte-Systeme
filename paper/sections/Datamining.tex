\chapter{Datamining}
\label{chap:datamining}

Dieses Kapitel unterscheidet verschiedene Möglichkeiten von Datamining im mobilen Bereich. Die zwei bekanntesten Standardverfahren zum allgemeinen Vorgehen bei Datamining -- sog. \textit{Mininprozesse} -- werden kurz vorgestellt. Anschließend werden zwei \textit{Learning Strategien} diskutiert und verschiedene Algorithmen, die diese Strategien umsetzen werden präsentiert.

\section{Mining Strategien}

Es gilt die in Kapitel \ref{chap:context} diskutierten Herausforderungen beim Datamining im mobilen Bereich effizient zu bewältigen. Insbesondere Rechenleistung und Akkulaufzeit sind kritische Faktoren, denn laut \cite{pocket2014} sind viele Rechenprozesse beim Datamining sehr resourcenhungrig. Darum ist es sinnvoll zu unterscheiden, wo (im physikalischen Sinne) die Miningprozesse stattfinden -- auf dem Gerät oder extern (zB. Cloud). In \cite{pocket2014} wird entferntes Mining als „mobile Interface“ Mining bezeichnet. Dabei wird das mobile Endgerät als Datenlieferant -- „Interface zu den Daten“ -- betrachtet. Alle erhobenen Daten werden an den entfernten Dienst gesendet und dort verarbeitet. Im Gegensatz dazu steht das „on-board Mining“. Dabei finden Dataminingprozesse direkt auf dem mobilen Gerät statt. Dies wurde erst mit zunehmender Leistung der Geräte in den letzten Jahren möglich \cite{pocket2014}.

\section{Mining Prozesse}

Der wohl bekannteste Miningprozess ist der 1996 vorgestellte „Knowledge Discovery in Databases“\cite{kdd96}, abgekürzt \textit{KDD}-Prozess. Dabei werden fünf Schritte durchlaufen, mit dem Ziel aus einer großen Menge Daten Wissen zu generieren. Wichtig ist zu beachten, dass die tatsächliche Ausfürhung von Datamining-Algorithmen nur einer der fünf Schritte ist. Das Verfahren beginnt mit einer Datenselektion, um die Menge der zu minenden Daten (ggf. drastisch) zu reduzieren. Anschließend folgt eine Vorverarbeitung und Datentransformation, um die ggf. heterogenen Daten mit einem gemeinsamen Verfahren bearbeitbar zu machen. Erst hiernach können Mining-Algorithmen angewandt werden. Die abschließende Interpretation der Miningergebnisse führt letztendlich zu neuem Wissen. Nach jedem Schritt ist es möglich, Zwischenergebnisse wieder in einen vorherigen Schritt zurückzuführen (siehe \cite{kdd96}).

Aufgund der Inflexibilität von KDD wurde 2000 ein anderer Standard entwickelt, dessen Ziel es ist, einen branchenübergreifenden Ansatz für Datamining zu formalisieren. Bei \textit{CRISP} -- „Cross Industry Standard Process for Datamining“\cite{crisp2000} handelt es sich um einen zyklischen Prozess zu dessen Beginn eine Analyse der Geschäftsanforderungen steht. Bereits hier wird der branchenübergreifende, weniger technische Ansatz sichtbar. Nach dem „Business Understanding“ folgen das „Data Understanding“, bei dem jene geschäftlich relevanten Daten analysiert werden und die „Data Preparation“. Im Gegensatz zum KDD werden hier die Schritte der Datenreduktion und Formatvereinheitlichung nicht extra erwähnt. Es folgt das „Modeling“ und die „Evaluation“, bei denen eine Problemstellung modelliert und anschließend evaluiert wird -- anhand von Datamining-Prozessen. Je nach Ausgang der Evaluation werden die gewonnenen Erkenntnisse angewandt („Deployment“) oder es findet ein Rückfluss zu dem „Understanding“ Phasen statt (siehe \cite{crisp2000}).

Nach \cite{pocket2014} ist der Schritt der Datenvorbereitung einer aufwändigsten Prozesse beim Datamining.

\section{Learning und Algorithmen}

Nach \cite{datamining2011} werden „supervised“ und „unsupervised“ Learning unterschieden. Beide beschreiben unterschiedliche Charakteristika für Datamining-Algorithmen. Beim \textit{supervised Learning} werden Datenpunkte durch den Menschen klassifiziert. Sowohl die Eingabedaten als auch die gewünschte Ausgabe des Algorithmus sind vorab bekannt. \textit{Supervised} Algorithmen verwenden den Klassifikator, der durch den Menschen spezifiziert wurde. Datenpunkte werden häufig in Trainingsdaten und Testdaten unterteilt, mit denen der Algorithmus zunächst angepasst und anschließend verifiziert wird.

Beim \textit{unsupervised} Learning hingegen wird der Klassifikator der Datenpunkte durch den Algorithmus bestimmt und ist nicht vorher durch den Menschen festgelegt. Außerdem sind nur die Eingabedaten bekannt, die Ausgabe nicht. Diese muss vielmehr zufriedenstellend für den Benutzer des Algorithmus sein, kann jedoch nicht festgelegt werden (siehe \cite{datamining2011}).

\subsection {Lineare Regression}

Ein bekannter Algorithmus, der \textit{supervised} ist, ist die \textit{lineare Regression}. Bei dieser werden alle Attribute einzelner Datenpunkte betrachtet. Der Algorithmus lernt, jedes einzelne Attribut zu wichten \cite{datamining2011}. Es wird eine Formel aufgestellt für einen Datenpunkt $x$, der Form
\[
	f(x) = w_0 + a_1w_1 + a_2w_2 + ... + a_kw_k
\]
\[
	a_k : Attribut\ k\ eines\ Datenpunktes
\]
\[
	w_k : Wichtung\ des\ Attributes\ k
\]

Mit der Funktion $f$ ist nun die Klassifikation von neuen Datenpunkten möglich. Der Algorithmus gilt als \textit{supervised}, weil die Klassifikation durch eine Formel erfolgt, die eben jene Attribute betrachtet die vorab bekannt sind.

\subsection{K-Nearest-Neighbor}

Bei dem \textit{K-Nearest-Neighbor} kurz \textit{KNN} handelt es sich ebenfalls um einen \textit{supervised} Algorithmus. KNN klassifiziert neue Datenpunkte dadurch, dass die $k$ nächsten Nachbarn des Punktes betrachtet werden, bei denen die Klassifikation bereits bekannt ist. Die Klassifikation erfolgt, indem dem neuen Punkt jene Klasse zugewiesen wird, die die meisten der $k$ nächsten Nachbarn haben \cite{datamining2011}. Abbildung \ref{fig:knn} zeigt die stark vereinfachte Klassifikation für das Attribut „Farbe“ der „roten Box“ mit Hilfe von KNN. Je nach Wahl des $k$ wird die rote Box entweder als \textit{blau} oder \textit{grün} klassifiziert.

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\textwidth]{../slides/img/knn}
    \caption{K-Nearest-Neighbor, vereinfachte Darstellung. Klassifikation der Roten Box: Wähle $K=5 \rightarrow$ \textit{blau}. Wähle $K=10 \rightarrow$ \textit{grün}\label{fig:knn}}
\end{figure}

\subsection{Support Vector Machine}

