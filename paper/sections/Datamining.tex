\chapter{Datamining}
\label{chap:datamining}

Dieses Kapitel unterscheidet verschiedene Möglichkeiten von Datamining im mobilen Bereich. Die zwei bekanntesten Standardverfahren zum allgemeinen Vorgehen bei Datamining -- sog. \textit{Mininprozesse} -- werden kurz vorgestellt. Anschließend werden zwei \textit{Learning Strategien} diskutiert und verschiedene Algorithmen, die diese Strategien umsetzen werden präsentiert.

\section{Mining Strategien}

Es gilt die in Kapitel \ref{chap:context} diskutierten Herausforderungen beim Datamining im mobilen Bereich effizient zu bewältigen. Insbesondere Rechenleistung und Akkulaufzeit sind kritische Faktoren, denn laut \cite{pocket2014} sind viele Rechenprozesse beim Datamining sehr resourcenhungrig. Darum ist es sinnvoll zu unterscheiden, wo (im physikalischen Sinne) die Miningprozesse stattfinden -- auf dem Gerät oder extern (zB. Cloud). In \cite{pocket2014} wird entferntes Mining als „mobile Interface“ Mining bezeichnet. Dabei wird das mobile Endgerät als Datenlieferant -- „Interface zu den Daten“ -- betrachtet. Alle erhobenen Daten werden an den entfernten Dienst gesendet und dort verarbeitet. Im Gegensatz dazu steht das „on-board Mining“. Dabei finden Dataminingprozesse direkt auf dem mobilen Gerät statt. Dies wurde erst mit zunehmender Leistung der Geräte in den letzten Jahren möglich \cite{pocket2014}.

\section{Mining Prozesse}

Der wohl bekannteste Miningprozess ist der 1996 vorgestellte „Knowledge Discovery in Databases“\cite{kdd96}, abgekürzt \textit{KDD}-Prozess. Dabei werden fünf Schritte durchlaufen, mit dem Ziel aus einer großen Menge Daten Wissen zu generieren. Wichtig ist zu beachten, dass die tatsächliche Ausfürhung von Datamining-Algorithmen nur einer der fünf Schritte ist. Das Verfahren beginnt mit einer Datenselektion, um die Menge der zu minenden Daten (ggf. drastisch) zu reduzieren. Anschließend folgt eine Vorverarbeitung und Datentransformation, um die ggf. heterogenen Daten mit einem gemeinsamen Verfahren bearbeitbar zu machen. Erst hiernach können Mining-Algorithmen angewandt werden. Die abschließende Interpretation der Miningergebnisse führt letztendlich zu neuem Wissen. Nach jedem Schritt ist es möglich, Zwischenergebnisse wieder in einen vorherigen Schritt zurückzuführen (siehe \cite{kdd96}).

Aufgund der Inflexibilität von KDD wurde 2000 ein anderer Standard entwickelt, dessen Ziel es ist, einen branchenübergreifenden Ansatz für Datamining zu formalisieren. Bei \textit{CRISP} -- „Cross Industry Standard Process for Datamining“\cite{crisp2000} handelt es sich um einen zyklischen Prozess zu dessen Beginn eine Analyse der Geschäftsanforderungen steht. Bereits hier wird der branchenübergreifende, weniger technische Ansatz sichtbar. Nach dem „Business Understanding“ folgen das „Data Understanding“, bei dem jene geschäftlich relevanten Daten analysiert werden und die „Data Preparation“. Im Gegensatz zum KDD werden hier die Schritte der Datenreduktion und Formatvereinheitlichung nicht extra erwähnt. Es folgt das „Modeling“ und die „Evaluation“, bei denen eine Problemstellung modelliert und anschließend evaluiert wird -- anhand von Datamining-Prozessen. Je nach Ausgang der Evaluation werden die gewonnenen Erkenntnisse angewandt („Deployment“) oder es findet ein Rückfluss zu dem „Understanding“ Phasen statt (siehe \cite{crisp2000}).

Nach \cite{pocket2014} ist der Schritt der Datenvorbereitung einer aufwändigsten Prozesse beim Datamining.

\section{Learning und Algorithmen}

Nach \cite{datamining2011} werden „supervised“ und „unsupervised“ Learning unterschieden. Beide beschreiben unterschiedliche Charakteristika für Datamining-Algorithmen. Beim \textit{supervised Learning} werden Datenpunkte durch den Menschen klassifiziert. Sowohl die Eingabedaten als auch die gewünschte Ausgabe des Algorithmus sind vorab bekannt. \textit{Supervised} Algorithmen verwenden den Klassifikator, der durch den Menschen spezifiziert wurde. Datenpunkte werden häufig in Trainingsdaten und Testdaten unterteilt, mit denen der Algorithmus zunächst angepasst und anschließend verifiziert wird.

Beim \textit{unsupervised} Learning hingegen wird der Klassifikator der Datenpunkte durch den Algorithmus bestimmt und ist nicht vorher durch den Menschen festgelegt. Außerdem sind nur die Eingabedaten bekannt, die Ausgabe nicht. Diese muss vielmehr zufriedenstellend für den Benutzer des Algorithmus sein, kann jedoch nicht festgelegt werden (siehe \cite{datamining2011}).

\subsection {Lineare Regression}

Ein bekannter Algorithmus, der \textit{supervised} ist, ist die \textit{lineare Regression}. Bei dieser werden alle Attribute einzelner Datenpunkte betrachtet. Der Algorithmus lernt, jedes einzelne Attribut zu wichten \cite{datamining2011}. Es wird eine Formel aufgestellt für einen Datenpunkt $x$, der Form
\[
	f(x) = w_0 + a_1w_1 + a_2w_2 + ... + a_kw_k
\]
\[
	a_k : Attribut\ k\ eines\ Datenpunktes
\]
\[
	w_k : Wichtung\ des\ Attributes\ k
\]

Mit der Funktion $f$ ist nun die Klassifikation von neuen Datenpunkten möglich. Der Algorithmus gilt als \textit{supervised}, weil die Klassifikation durch eine Formel erfolgt, die eben jene Attribute betrachtet die vorab bekannt sind.

\subsection{K-Nearest-Neighbor}

Bei dem \textit{K-Nearest-Neighbor} kurz \textit{KNN} handelt es sich ebenfalls um einen \textit{supervised} Algorithmus. KNN klassifiziert neue Datenpunkte dadurch, dass die $k$ nächsten Nachbarn des Punktes betrachtet werden, bei denen die Klassifikation bereits bekannt ist. Die Klassifikation erfolgt, indem dem neuen Punkt jene Klasse zugewiesen wird, die die meisten der $k$ nächsten Nachbarn haben \cite{datamining2011}. Abbildung \ref{fig:knn} zeigt die stark vereinfachte Klassifikation für das Attribut „Farbe“ der „roten Box“ mit Hilfe von KNN. Je nach Wahl des $k$ wird die rote Box entweder als \textit{blau} oder \textit{grün} klassifiziert.

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\textwidth]{../slides/img/knn}
    \caption{K-Nearest-Neighbor, vereinfachte Darstellung. Klassifikation der Roten Box: Wähle $K=5 \rightarrow$ \textit{blau}. Wähle $K=10 \rightarrow$ \textit{grün}\label{fig:knn}}
\end{figure}

\subsection{Support Vector Machine}

Eine Support Vector Machine -- kurz \textit{SVM} -- betrachtet eine Menge kritischer Datenpunkte im Datenraum und versucht diese Punkte soweit entfernt voneinander wie möglich zu unterteilen \cite{datamining2011}. Die daraus entstehende Unterscheidungsfunktion wird als \textit{Hyperplane} bezeichnet. Sie ist in ihrer Dimensionalität um eins geringer als die zu unterteilende Punktewolke, im einfachsten Fall also linear. Die Klassifikation eines neuen Punktes mithilfe einer SVM geschieht, indem von der SVM entschieden wird, an welcher Stelle des unterteilten Punkteraums der neue Punkt sich befindet \cite{datamining2011}. Abbildung \ref{fig:svm} zeigt ein einfaches Beispiel, bei dem die drei Hyperplanes \textit{A, B} und \textit{C} den Punkteraum unterteilen. \textit{B} wird als bester Klassifikator betrachtet, da \textit{B} die existierenden Punkte soweit wie möglich von einander entfernt unterteilt.

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{../slides/img/SVM_21}
    \caption{Online Material von \cite{ray2015}, \url{https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/}\label{fig:svm}}
\end{figure}

Auch die SVM zählt zu den \textit{supervised} Verfahren, denn die Klassifikation begründet sich auf der bereits bekannten Klassifikation der existierenden Punkte.

\subsection{K-Means-Clustering}

K-Means ist ein bekanntes Clustering Verfahren. Es gilt als \textit{unsupervised}, denn der Algorithmus benötigt keine bereits bekannten Attribute zur Klassifikation. Der K-Means Algorithmus unterteilt den Punkteraum derart, dass $k$ Cluster gebildet werden. Das Attribut der Klassifikation ist die Distanz der einzelnen Punkte im Raum zu einander \cite{datamining2011}. Listing \ref{lst:kmeans} zeigt exemplarischen Pseudocode, um den Ablauf des Algorithmus zu veranschaulichen.

\begin{center}
\begin{lstlisting}[caption={K-Means Algorithmus \cite{lavrenko2013}},label=lst:kmeans,language=JAVA,label={lst:kmeans}]
def K-Means(K: int, toCluster: Set):
    Place K centroids randomly
    repeat until convergence:
        for each Point in 'toCluster':
            find closest centroid
            add point to cluster
        for each cluster in 1..K:
            new centroid: mid of all points
\end{lstlisting}
\end{center}

Die einzelnen Schritte des Algorithmus werden in der folgenden Grafik \ref{fig:kmeans} für $k = 2$ illustiert.

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{../slides/img/kmeans}
    \caption{Online Material von \cite{lavrenko2013}, \url{https://i.ytimg.com/vi/_aWzGGNrcic/hqdefault.jpg}\label{fig:kmeans}}
\end{figure}

Die Cluster Mittelpunkte, die initial zufällig vom Algorithmus bestimmt werden, sind als Dreiecke dargestellt. Die gestrichelte rote Linie zeigt die mittlere Distanz zwischen den Zentroiden und veranschaulicht, welche Punkte welchem Zentroiden zugewiesen werden. Nach jeder Iteration werden die Zentroide verschoben, sodass sie wieder im Mittelpunkte des Clusters liegen, bis schließlich der Algorithmus terminiert und die Zentroide sich nicht mehr ändern. Die Klassifikation eines Datenpunktes geschieht nun über die Zugehörigkeit des Punktes zu einem der gebildeten $k$-Cluster. In der Grafik ist dies unten links zu sehen.